{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMW6fzxb24Un1vjSEVoGHjM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dbckz/aoc-2022/blob/main/AOC_day_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Me"
      ],
      "metadata": {
        "id": "R3LqeXDlDhOD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "jMQWvukZDacD"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Rucksack:\n",
        "  def __init__(self, input_string):\n",
        "    self.input_string = input_string\n",
        "    self.comp_one = input_string[:int(len(input_string) * 0.5)]\n",
        "    self.comp_two = input_string[int(len(input_string) * 0.5):]\n",
        "    self.wrong_item = self.get_wrong_item()\n",
        "    self.wrong_item_priority = self.get_priority()\n",
        "\n",
        "  def get_wrong_item(self):\n",
        "    for c in self.comp_one:\n",
        "      if c in self.comp_two:\n",
        "        return c\n",
        "\n",
        "  def get_priority(self):\n",
        "    if self.wrong_item.islower():\n",
        "      return ord(self.wrong_item) - 96\n",
        "    else:\n",
        "      return ord(self.wrong_item) - 38\n"
      ],
      "metadata": {
        "id": "prmIFcs-ERso"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rucksacks = []\n",
        "\n",
        "with open('input') as f:\n",
        "  for line in f:\n",
        "    r = Rucksack(line.strip())\n",
        "    rucksacks.append(r)"
      ],
      "metadata": {
        "id": "ENZ-1EjxDjuu"
      },
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = sum([r.wrong_item_priority for r in rucksacks])\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s8ikPuKEHRoZ",
        "outputId": "cd6741fb-3a26-4073-b408-f62cbf3bd0b6"
      },
      "execution_count": 61,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Group:\n",
        "  def __init__(self, rucksacks):\n",
        "    self.rucksacks = rucksacks\n",
        "    self.badge = self.get_badge()\n",
        "    self.badge_priority = self.get_priority()\n",
        "\n",
        "  def get_badge(self):\n",
        "    for c in self.rucksacks[0].input_string:\n",
        "      if ((c in self.rucksacks[1].input_string) and (c in self.rucksacks[2].input_string)):\n",
        "        return c\n",
        "\n",
        "  def get_priority(self):\n",
        "    if self.badge.islower():\n",
        "      return ord(self.badge) - 96\n",
        "    else:\n",
        "      return ord(self.badge) - 38\n"
      ],
      "metadata": {
        "id": "_uWq8z-ZJK7X"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "groups = []\n",
        "\n",
        "for i in range(len(rucksacks))[::3]:\n",
        "  group = Group(rucksacks[i:i+3])\n",
        "  groups.append(group)"
      ],
      "metadata": {
        "id": "l3fVL7RUKTeV"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "answer = sum([g.badge_priority for g in groups])\n",
        "print(answer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-HQfexdLNAQu",
        "outputId": "9ea07d4a-f575-4d42-d3ba-1ea218eaf0f0"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# GPT-3"
      ],
      "metadata": {
        "id": "DJZCmZYENTe6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Solution \n",
        "\n",
        "# Define a function to calculate the priority of an item type\n",
        "def calculatePriority(itemType): \n",
        "  if itemType.isupper(): \n",
        "    priority = ord(itemType) - ord('A') + 27 \n",
        "  elif itemType.islower(): \n",
        "    priority = ord(itemType) - ord('a') + 1 \n",
        "  else: \n",
        "    priority = 0 \n",
        "  return priority \n",
        "\n",
        "# Input - (this bit added by me, as GPT just generated it's own example. It has no knowledge of how input is passed, of course)\n",
        "rucksacks = []\n",
        "\n",
        "with open('input') as f:\n",
        "  for line in f:\n",
        "    r = line.strip()\n",
        "    rucksacks.append(r)\n",
        "\n",
        "# Initialize variables \n",
        "itemTypes = [] \n",
        "priorities = [] \n",
        "\n",
        "# Iterate through each rucksack \n",
        "for rucksack in rucksacks: \n",
        "  # Get the items in the first compartment \n",
        "  firstCompartmentItems = rucksack[:len(rucksack)//2] \n",
        "  # Get the items in the second compartment \n",
        "  secondCompartmentItems = rucksack[len(rucksack)//2:] \n",
        "  # Iterate through each item in the first compartment \n",
        "  #\n",
        "  # By me - GPT solution has a bug here - it doesn't account for an item type appearing more than once in\n",
        "  # a compartment. This means it overcounts. To fix, I've added a 'break' to break out of the loop\n",
        "  # as soon as it finds a match. Quite a subtle bug to find - shows potential challenges in debugging\n",
        "  # code that you didn't write!\n",
        "  #\n",
        "  # Rather than add the break myself, I can get GTP to do it correctly by adding an explicit line in the\n",
        "  # the prompt telling it to only count multiple appearances once. A small example of AI misalignment :)\n",
        "  for item in firstCompartmentItems:\n",
        "    # Check if the item is in the second compartment \n",
        "    if item in secondCompartmentItems: \n",
        "      # Get the item type \n",
        "      itemType = item\n",
        "      # Append the item type to the list \n",
        "      itemTypes.append(itemType) \n",
        "      # Calculate the priority of the item type \n",
        "      priority = calculatePriority(itemType) \n",
        "      # Append the priority to the list \n",
        "      priorities.append(priority)\n",
        "      break # line added by me \n",
        "\n",
        "# Print the list of item types \n",
        "print('Item types:', itemTypes) \n",
        "\n",
        "# Calculate the sum of the priorities \n",
        "sumOfPriorities = sum(priorities) \n",
        "\n",
        "# Print the sum of the priorities \n",
        "print('Sum of priorities:', sumOfPriorities)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhGQ6wu9NVL1",
        "outputId": "1326a48f-6e99-4482-da7c-9a536e385b04"
      },
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Item types: ['d', 'Z', 'P', 'N', 'q', 'W', 'd', 'S', 'P', 'm', 't', 'T', 'B', 'q', 'G', 'P', 'G', 'g', 's', 'z', 'C', 'p', 'N', 'H', 'w', 'H', 'Z', 'D', 'Q', 'C', 'f', 'N', 'l', 'D', 'v', 'T', 'Q', 's', 'v', 'W', 'C', 'd', 'j', 'V', 'm', 'B', 'R', 'b', 'C', 'l', 'j', 'C', 'm', 'L', 'c', 'T', 'V', 'G', 'D', 's', 'N', 'Z', 's', 'R', 'G', 'B', 'b', 'r', 'D', 'r', 'q', 'd', 'H', 'L', 'b', 'p', 'm', 'h', 'w', 'S', 'h', 'c', 'T', 'L', 'B', 'N', 'V', 'B', 'S', 'q', 'R', 'h', 'M', 'p', 'L', 'R', 'S', 'V', 'l', 'd', 'N', 'P', 'g', 'Q', 'f', 'g', 'f', 'v', 'H', 'p', 'R', 'L', 's', 'm', 'P', 'H', 'f', 'B', 'C', 'p', 'H', 'r', 'w', 'm', 'G', 'P', 'r', 'v', 'b', 'N', 'R', 'C', 'B', 'C', 'f', 'C', 'R', 'L', 's', 'P', 'S', 'z', 'N', 'H', 'r', 'f', 'C', 'm', 'C', 'Z', 'G', 'n', 'W', 'F', 'r', 't', 'g', 'Q', 'H', 'j', 'H', 'G', 'b', 'g', 'Z', 'm', 'P', 'q', 't', 'm', 'f', 'T', 'q', 'J', 'j', 'z', 'S', 'D', 'H', 'F', 'G', 'Q', 'h', 'h', 'n', 'Z', 'W', 'C', 'M', 'Z', 'R', 'n', 'r', 'l', 'Z', 'w', 'V', 'Q', 'Q', 'm', 't', 'h', 'n', 'G', 'L', 'g', 's', 'j', 'N', 'Z', 'J', 'T', 'g', 'J', 'B', 'D', 'n', 'v', 'C', 'Z', 'v', 'z', 'p', 'W', 'C', 'j', 'h', 'c', 'w', 'j', 'n', 'R', 'Q', 'H', 'G', 'J', 'b', 'd', 'f', 's', 'g', 'P', 'C', 'B', 'W', 'J', 'M', 's', 'd', 'W', 'v', 'M', 'g', 'q', 'C', 'M', 'B', 'Z', 'c', 'Q', 'Z', 'P', 'p', 'f', 'H', 'g', 'F', 'Q', 'N', 'L', 'd', 'W', 'n', 'c', 'z', 'n', 'Z', 'G', 'P', 'N', 'D', 'G', 'd', 'R', 'P', 'W', 'S', 'N', 'l', 'B', 'w', 'c', 'J', 'B', 'j', 'M', 'r', 'P', 'w', 'J']\n",
            "Sum of priorities: 8243\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IK9Q4BsjSGQ2"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "I did not have enough token space in my prompt to do part 2. So I did it in https://chat.openai.com/ instead, as it appears this has 'memory' through the conversation. Whereas on the beta playground, starting part 2 in a new prompt loses all context of part 1. Would have to significantly change the prompt from what AOC provide, which I can't be bothered to do.\n",
        "\n",
        "Annoyingly, it looks like there's not built-in functionality to save and link to a chat, but below is the code generated.\n",
        "\n",
        "Also seems like chat is WAY quicker, and the interface is much nicer - you are returned a code block that you can one-click copy :)"
      ],
      "metadata": {
        "id": "kbIMgEBTSuS9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_common_item_type(rucksack1, rucksack2, rucksack3):\n",
        "    # initialize a set to keep track of common item types\n",
        "    common_item_types = set()\n",
        "    # get the first half of the first rucksack\n",
        "    first_half = rucksack1[:len(rucksack1)//2]\n",
        "    # iterate over the characters in the first half\n",
        "    for c in first_half:\n",
        "        # check if the character is in the second and third rucksacks\n",
        "        if c in rucksack2 and c in rucksack3:\n",
        "            # if it is, add it to the set of common item types\n",
        "            common_item_types.add(c)\n",
        "    # get the second half of the first rucksack\n",
        "    second_half = rucksack1[len(rucksack1)//2:]\n",
        "    # iterate over the characters in the second half\n",
        "    for c in second_half:\n",
        "        # check if the character is in the second and third rucksacks\n",
        "        if c in rucksack2 and c in rucksack3:\n",
        "            # if it is, add it to the set of common item types\n",
        "            common_item_types.add(c)\n",
        "    # return the set of common item types\n",
        "    return common_item_types\n",
        "\n",
        "def get_priority(item_type):\n",
        "    # if the item type is lowercase, its priority is the ordinal value of the character - 96\n",
        "    if item_type.islower():\n",
        "        return ord(item_type) - 96\n",
        "    # if the item type is uppercase, its priority is the ordinal value of the character - 38\n",
        "    else:\n",
        "        return ord(item_type) - 38\n",
        "\n",
        "def sum_priorities(rucksacks):\n",
        "    # initialize the sum to 0\n",
        "    result = 0\n",
        "    # iterate over the rucksacks in groups of three\n",
        "    for i in range(0, len(rucksacks), 3):\n",
        "        # get the common item types for the current group\n",
        "        common_item_types = get_common_item_type(\n",
        "            rucksacks[i],\n",
        "            rucksacks[i+1],\n",
        "            rucksacks[i+2]\n",
        "        )\n",
        "        # add the priorities of the common item types to the result\n",
        "        for item_type in common_item_types:\n",
        "            result += get_priority(item_type)\n",
        "    # return the result\n",
        "    return result\n",
        "\n",
        "# Me - input function added by me again\n",
        "rucksacks = []\n",
        "\n",
        "with open('input') as f:\n",
        "  for line in f:\n",
        "    r = line.strip()\n",
        "    rucksacks.append(r)\n",
        "\n",
        "# get the sum of the priorities\n",
        "result = sum_priorities(rucksacks)\n",
        "\n",
        "# print the result\n",
        "print(result)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WeRXQW5KUXH5",
        "outputId": "5e74893c-b2ff-4ada-bb5b-2965aa4414e1"
      },
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2631\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Also just found you can ask it to use e.g. NumPy rather than native python lists, and it does it. Pretty neat."
      ],
      "metadata": {
        "id": "nPelj6igVAKI"
      }
    }
  ]
}